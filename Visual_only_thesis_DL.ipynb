{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 12259390,
          "sourceType": "datasetVersion",
          "datasetId": 7725149
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Visual_only_thesis_DL",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "ObHUZA9LZRDc"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "khadiza13_less_data_path = kagglehub.dataset_download('khadiza13/less-data')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "G4BDdEIqZRDf"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T08:11:03.670449Z",
          "iopub.execute_input": "2025-06-24T08:11:03.670659Z",
          "iopub.status.idle": "2025-06-24T08:11:05.043506Z",
          "shell.execute_reply.started": "2025-06-24T08:11:03.670642Z",
          "shell.execute_reply": "2025-06-24T08:11:05.042517Z"
        },
        "id": "QwCuGgazZRDg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/kaggle/input/less-data/changed_ds/new_ds_small.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T08:11:52.378912Z",
          "iopub.execute_input": "2025-06-24T08:11:52.379152Z",
          "iopub.status.idle": "2025-06-24T08:11:52.452683Z",
          "shell.execute_reply.started": "2025-06-24T08:11:52.379134Z",
          "shell.execute_reply": "2025-06-24T08:11:52.451931Z"
        },
        "id": "0K--kklCZRDg",
        "outputId": "3dae8273-fb19-4894-ccda-1956e6a838f3"
      },
      "outputs": [
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "      image                                               text  \\\n0  2001.jpg  আচ্ছা ভাই।\\n মেয়েদের থেকে দূরে থাকবা মেয়ের পাল...   \n1  2002.jpg  কিসের foodpanda যেখানে আমার সাদিয়া রে অর্ডার ক...   \n2  2003.jpg  উপস্থিত sir\\n Yes sir\\n Present Teacher\\n [লাব...   \n3  2004.png  আমি হিজাব চাইনি\\n তারপর আমার স্বামী আমাকে বোঝালেন   \n4  2005.jpg  বাঙ্গু (beta, omega, theta, delta) male\\nসুন্দ...   \n\n                          label  \n0  stereotype & objectification  \n1  stereotype & objectification  \n2  stereotype & objectification  \n3             violence or abuse  \n4              non-misogynistic  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2001.jpg</td>\n      <td>আচ্ছা ভাই।\\n মেয়েদের থেকে দূরে থাকবা মেয়ের পাল...</td>\n      <td>stereotype &amp; objectification</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2002.jpg</td>\n      <td>কিসের foodpanda যেখানে আমার সাদিয়া রে অর্ডার ক...</td>\n      <td>stereotype &amp; objectification</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003.jpg</td>\n      <td>উপস্থিত sir\\n Yes sir\\n Present Teacher\\n [লাব...</td>\n      <td>stereotype &amp; objectification</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2004.png</td>\n      <td>আমি হিজাব চাইনি\\n তারপর আমার স্বামী আমাকে বোঝালেন</td>\n      <td>violence or abuse</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2005.jpg</td>\n      <td>বাঙ্গু (beta, omega, theta, delta) male\\nসুন্দ...</td>\n      <td>non-misogynistic</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    random.seed(seed)\n",
        "set_seed(42)\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/kaggle/input/less-data/changed_ds/new_ds_small.csv\")\n",
        "assert not df.empty, \"Dataset is empty\"\n",
        "assert len(df) == 4001, f\"Expected 4001 samples, got {len(df)}\"\n",
        "assert df['image'].notnull().all(), \"Missing values in 'image' column\"\n",
        "assert df['label'].notnull().all(), \"Missing values in 'label' column\"\n",
        "print(\"✅ Dataset loaded. Sample:\")\n",
        "print(df.head())\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['label_id'] = label_encoder.fit_transform(df['label'])\n",
        "label2id = {label: idx for idx, label in enumerate(label_encoder.classes_)}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "print(f\"✅ Unique labels: {label2id}\")\n",
        "print(f\"Label distribution: {df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "# Split data into train (70%), validation (15%), test (15%)\n",
        "train_df, temp_df = train_test_split(\n",
        "    df, test_size=0.3, random_state=42, stratify=df['label_id']\n",
        ")\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.5, random_state=42, stratify=temp_df['label_id']\n",
        ")\n",
        "print(f\"✅ Dataset split: Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
        "\n",
        "# Image dataset class\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = df['image'].values\n",
        "        self.labels = df['label_id'].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.image_paths[idx])\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            image = Image.new('RGB', (224, 224))  # Fallback: blank image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return {'image': image, 'labels': label}\n",
        "\n",
        "# Image preprocessing\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Dataset and DataLoader\n",
        "img_dir = \"/kaggle/input/less-data/changed_ds/img\"\n",
        "train_dataset = ImageDataset(train_df, img_dir, transform=train_transform)\n",
        "val_dataset = ImageDataset(val_df, img_dir, transform=val_test_transform)\n",
        "test_dataset = ImageDataset(test_df, img_dir, transform=val_test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Define custom CNN\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Model definitions\n",
        "def get_model(model_name, num_classes):\n",
        "    if model_name == \"CustomCNN\":\n",
        "        model = CustomCNN(num_classes)\n",
        "    elif model_name == \"ResNet50\":\n",
        "        model = models.resnet50(pretrained=True)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    elif model_name == \"VGG16\":\n",
        "        model = models.vgg16(pretrained=True)\n",
        "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "    elif model_name == \"VGG19\":\n",
        "        model = models.vgg19(pretrained=True)\n",
        "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "    return model\n",
        "\n",
        "# Training and evaluation function\n",
        "def train_and_evaluate(model_name, model, train_loader, val_loader, test_loader, num_classes, device):\n",
        "    print(f\"\\n🔄 Training {model_name}...\")\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 3\n",
        "    min_delta = 0.01\n",
        "    best_f1 = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    num_epochs = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        # Progress bar for training\n",
        "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\")\n",
        "        for batch in train_bar:\n",
        "            images = batch['image'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            train_bar.set_postfix({\"loss\": running_loss / len(train_bar)})\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_preds, val_labels = [], []\n",
        "        val_loss = 0.0\n",
        "        # Progress bar for validation\n",
        "        val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\")\n",
        "        with torch.no_grad():\n",
        "            for batch in val_bar:\n",
        "                images = batch['image'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                val_preds.extend(preds.cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "                val_bar.set_postfix({\"val_loss\": val_loss / len(val_bar)})\n",
        "\n",
        "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
        "        print(f\"Epoch {epoch+1}: Val F1 = {val_f1:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_f1 > best_f1 + min_delta:\n",
        "            best_f1 = val_f1\n",
        "            patience_counter = 0\n",
        "            os.makedirs(f\"./models/{model_name}\", exist_ok=True)\n",
        "            torch.save(model.state_dict(), f\"./models/{model_name}/best_model.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(f\"./models/{model_name}/best_model.pt\"))\n",
        "\n",
        "    # Validation evaluation\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    # Progress bar for validation evaluation\n",
        "    val_bar = tqdm(val_loader, desc=\"Validation Evaluation\")\n",
        "    with torch.no_grad():\n",
        "        for batch in val_bar:\n",
        "            images = batch['image'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(f\"\\n📊 Validation Classification Report for {model_name}\")\n",
        "    val_report = classification_report(val_labels, val_preds, target_names=label_encoder.classes_)\n",
        "    print(val_report)\n",
        "\n",
        "    # Test evaluation\n",
        "    test_preds, test_labels = [], []\n",
        "    # Progress bar for test evaluation\n",
        "    test_bar = tqdm(test_loader, desc=\"Test Evaluation\")\n",
        "    with torch.no_grad():\n",
        "        for batch in test_bar:\n",
        "            images = batch['image'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            test_preds.extend(preds.cpu().numpy())\n",
        "            test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(f\"\\n📊 Test Classification Report for {model_name}\")\n",
        "    test_report = classification_report(test_labels, test_preds, target_names=label_encoder.classes_)\n",
        "    print(test_report)\n",
        "\n",
        "    # Save results\n",
        "    results[model_name] = {\n",
        "        \"val_report\": classification_report(val_labels, val_preds, target_names=label_encoder.classes_, output_dict=True),\n",
        "        \"test_report\": classification_report(test_labels, test_preds, target_names=label_encoder.classes_, output_dict=True)\n",
        "    }\n",
        "\n",
        "    # Clear memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"🧹 Cleared memory after {model_name}\")\n",
        "\n",
        "# Main execution\n",
        "results = {}\n",
        "model_names = [\"CustomCNN\", \"ResNet50\", \"VGG16\", \"VGG19\"]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Train and evaluate models\n",
        "for model_name in model_names:\n",
        "    try:\n",
        "        model = get_model(model_name, num_classes=len(label2id))\n",
        "        train_and_evaluate(model_name, model, train_loader, val_loader, test_loader, len(label2id), device)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Skipping {model_name} due to error: {e}\")\n",
        "\n",
        "# Compare results\n",
        "print(\"\\n📊 Model Comparison (Test Metrics)\")\n",
        "for model_name, result in results.items():\n",
        "    test_acc = result[\"test_report\"][\"accuracy\"]\n",
        "    test_f1 = result[\"test_report\"][\"weighted avg\"][\"f1-score\"]\n",
        "    print(f\"{model_name}: Accuracy = {test_acc:.4f}, F1 = {test_f1:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T08:19:59.959493Z",
          "iopub.execute_input": "2025-06-24T08:19:59.960281Z",
          "iopub.status.idle": "2025-06-24T08:59:19.935845Z",
          "shell.execute_reply.started": "2025-06-24T08:19:59.960254Z",
          "shell.execute_reply": "2025-06-24T08:59:19.935282Z"
        },
        "id": "3caWL6jUZRDh",
        "outputId": "f3b7ec9b-90df-4d22-fd66-c39af68fab9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Dataset loaded. Sample:\n      image                                               text  \\\n0  2001.jpg  আচ্ছা ভাই।\\n মেয়েদের থেকে দূরে থাকবা মেয়ের পাল...   \n1  2002.jpg  কিসের foodpanda যেখানে আমার সাদিয়া রে অর্ডার ক...   \n2  2003.jpg  উপস্থিত sir\\n Yes sir\\n Present Teacher\\n [লাব...   \n3  2004.png  আমি হিজাব চাইনি\\n তারপর আমার স্বামী আমাকে বোঝালেন   \n4  2005.jpg  বাঙ্গু (beta, omega, theta, delta) male\\nসুন্দ...   \n\n                          label  \n0  stereotype & objectification  \n1  stereotype & objectification  \n2  stereotype & objectification  \n3             violence or abuse  \n4              non-misogynistic  \n✅ Unique labels: {'non-misogynistic': 0, 'stereotype & objectification': 1, 'violence or abuse': 2}\nLabel distribution: {'stereotype & objectification': 1591, 'non-misogynistic': 1380, 'violence or abuse': 1030}\n✅ Dataset split: Train: 2800, Val: 600, Test: 601\nUsing device: cuda\n\n🔄 Training CustomCNN...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 1 Training:  60%|██████    | 105/175 [00:27<00:17,  3.94it/s, loss=0.662]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 1 Training: 100%|██████████| 175/175 [00:45<00:00,  3.85it/s, loss=1.08] \nEpoch 1 Validation: 100%|██████████| 38/38 [00:08<00:00,  4.55it/s, val_loss=1.01] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1: Val F1 = 0.3881\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2 Training:  24%|██▍       | 42/175 [00:10<00:35,  3.76it/s, loss=0.249] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 2 Training: 100%|██████████| 175/175 [00:45<00:00,  3.87it/s, loss=1.03] \nEpoch 2 Validation: 100%|██████████| 38/38 [00:08<00:00,  4.48it/s, val_loss=0.99] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2: Val F1 = 0.4122\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3 Training:  35%|███▌      | 62/175 [00:15<00:30,  3.76it/s, loss=0.346] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 3 Training: 100%|██████████| 175/175 [00:44<00:00,  3.92it/s, loss=0.995]\nEpoch 3 Validation: 100%|██████████| 38/38 [00:08<00:00,  4.67it/s, val_loss=0.983]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3: Val F1 = 0.4998\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4 Training:  59%|█████▉    | 103/175 [00:26<00:18,  3.88it/s, loss=0.57] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 4 Training: 100%|██████████| 175/175 [00:44<00:00,  3.90it/s, loss=0.967]\nEpoch 4 Validation: 100%|██████████| 38/38 [00:08<00:00,  4.66it/s, val_loss=0.954]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4: Val F1 = 0.4675\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5 Training:  17%|█▋        | 29/175 [00:07<00:36,  3.99it/s, loss=0.149] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 5 Training: 100%|██████████| 175/175 [00:44<00:00,  3.95it/s, loss=0.944]\nEpoch 5 Validation: 100%|██████████| 38/38 [00:08<00:00,  4.55it/s, val_loss=0.951]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5: Val F1 = 0.4992\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6 Training:  73%|███████▎  | 128/175 [00:32<00:11,  3.94it/s, loss=0.673]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 6 Training: 100%|██████████| 175/175 [00:44<00:00,  3.95it/s, loss=0.921]\nEpoch 6 Validation: 100%|██████████| 38/38 [00:08<00:00,  4.57it/s, val_loss=0.951]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 6: Val F1 = 0.5434\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7 Training:  81%|████████  | 142/175 [00:37<00:09,  3.31it/s, loss=0.722]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 7 Training: 100%|██████████| 175/175 [00:45<00:00,  3.87it/s, loss=0.892]\nEpoch 7 Validation: 100%|██████████| 38/38 [00:09<00:00,  4.16it/s, val_loss=0.922]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 7: Val F1 = 0.5763\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 8 Training:  71%|███████   | 124/175 [00:33<00:14,  3.49it/s, loss=0.608]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 8 Training: 100%|██████████| 175/175 [00:47<00:00,  3.68it/s, loss=0.85] \nEpoch 8 Validation: 100%|██████████| 38/38 [00:08<00:00,  4.38it/s, val_loss=0.929]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 8: Val F1 = 0.5520\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 9 Training:  22%|██▏       | 38/175 [00:09<00:36,  3.76it/s, loss=0.176] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 9 Training: 100%|██████████| 175/175 [00:45<00:00,  3.83it/s, loss=0.819]\nEpoch 9 Validation: 100%|██████████| 38/38 [00:08<00:00,  4.54it/s, val_loss=0.98] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 9: Val F1 = 0.5373\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 10 Training:  69%|██████▉   | 121/175 [00:31<00:12,  4.22it/s, loss=0.54] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 10 Training: 100%|██████████| 175/175 [00:44<00:00,  3.93it/s, loss=0.787]\nEpoch 10 Validation: 100%|██████████| 38/38 [00:08<00:00,  4.64it/s, val_loss=0.947]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 10: Val F1 = 0.5561\nEarly stopping at epoch 10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Validation Evaluation: 100%|██████████| 38/38 [00:08<00:00,  4.47it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n📊 Validation Classification Report for CustomCNN\n                              precision    recall  f1-score   support\n\n            non-misogynistic       0.62      0.71      0.66       207\nstereotype & objectification       0.58      0.58      0.58       239\n           violence or abuse       0.51      0.41      0.45       154\n\n                    accuracy                           0.58       600\n                   macro avg       0.57      0.57      0.57       600\n                weighted avg       0.58      0.58      0.58       600\n\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Test Evaluation: 100%|██████████| 38/38 [00:15<00:00,  2.49it/s]\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n📊 Test Classification Report for CustomCNN\n                              precision    recall  f1-score   support\n\n            non-misogynistic       0.54      0.69      0.61       207\nstereotype & objectification       0.54      0.51      0.52       239\n           violence or abuse       0.43      0.31      0.36       155\n\n                    accuracy                           0.52       601\n                   macro avg       0.50      0.50      0.50       601\n                weighted avg       0.51      0.52      0.51       601\n\n🧹 Cleared memory after CustomCNN\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 132MB/s] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n🔄 Training ResNet50...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 1 Training:  82%|████████▏ | 143/175 [00:49<00:11,  2.87it/s, loss=0.801]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 1 Training: 100%|██████████| 175/175 [00:59<00:00,  2.96it/s, loss=0.982]\nEpoch 1 Validation: 100%|██████████| 38/38 [00:09<00:00,  4.14it/s, val_loss=0.936]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1: Val F1 = 0.5483\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2 Training:   8%|▊         | 14/175 [00:04<00:51,  3.10it/s, loss=0.0585]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 2 Training: 100%|██████████| 175/175 [00:56<00:00,  3.08it/s, loss=0.807]\nEpoch 2 Validation: 100%|██████████| 38/38 [00:09<00:00,  4.18it/s, val_loss=0.887]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2: Val F1 = 0.6016\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3 Training:  24%|██▍       | 42/175 [00:13<00:43,  3.07it/s, loss=0.161] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 3 Training: 100%|██████████| 175/175 [00:56<00:00,  3.08it/s, loss=0.683]\nEpoch 3 Validation: 100%|██████████| 38/38 [00:09<00:00,  4.11it/s, val_loss=0.939]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3: Val F1 = 0.6020\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4 Training:  59%|█████▉    | 104/175 [00:33<00:23,  3.04it/s, loss=0.329]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 4 Training: 100%|██████████| 175/175 [00:57<00:00,  3.06it/s, loss=0.566]\nEpoch 4 Validation: 100%|██████████| 38/38 [00:09<00:00,  4.12it/s, val_loss=1.05] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4: Val F1 = 0.6015\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5 Training:  74%|███████▎  | 129/175 [00:42<00:16,  2.84it/s, loss=0.322]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 5 Training: 100%|██████████| 175/175 [00:56<00:00,  3.08it/s, loss=0.455]\nEpoch 5 Validation: 100%|██████████| 38/38 [00:09<00:00,  4.06it/s, val_loss=1.16] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5: Val F1 = 0.5549\nEarly stopping at epoch 5\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Validation Evaluation: 100%|██████████| 38/38 [00:10<00:00,  3.67it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n📊 Validation Classification Report for ResNet50\n                              precision    recall  f1-score   support\n\n            non-misogynistic       0.67      0.72      0.69       207\nstereotype & objectification       0.60      0.67      0.63       239\n           violence or abuse       0.53      0.37      0.44       154\n\n                    accuracy                           0.61       600\n                   macro avg       0.60      0.59      0.59       600\n                weighted avg       0.60      0.61      0.60       600\n\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Test Evaluation: 100%|██████████| 38/38 [00:12<00:00,  3.14it/s]\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n📊 Test Classification Report for ResNet50\n                              precision    recall  f1-score   support\n\n            non-misogynistic       0.65      0.72      0.68       207\nstereotype & objectification       0.60      0.69      0.64       239\n           violence or abuse       0.64      0.39      0.49       155\n\n                    accuracy                           0.62       601\n                   macro avg       0.63      0.60      0.60       601\n                weighted avg       0.62      0.62      0.61       601\n\n🧹 Cleared memory after ResNet50\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:03<00:00, 151MB/s]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n🔄 Training VGG16...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 1 Training:  93%|█████████▎| 163/175 [01:05<00:04,  2.76it/s, loss=0.989]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 1 Training: 100%|██████████| 175/175 [01:10<00:00,  2.49it/s, loss=1.05] \nEpoch 1 Validation: 100%|██████████| 38/38 [00:09<00:00,  3.94it/s, val_loss=1.02] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1: Val F1 = 0.4210\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2 Training:  56%|█████▌    | 98/175 [00:36<00:25,  3.07it/s, loss=0.545] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 2 Training: 100%|██████████| 175/175 [01:05<00:00,  2.69it/s, loss=0.988]\nEpoch 2 Validation: 100%|██████████| 38/38 [00:09<00:00,  3.95it/s, val_loss=1.01] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2: Val F1 = 0.4168\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3 Training:  78%|███████▊  | 137/175 [00:50<00:13,  2.71it/s, loss=0.746]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 3 Training: 100%|██████████| 175/175 [01:04<00:00,  2.71it/s, loss=0.946]\nEpoch 3 Validation: 100%|██████████| 38/38 [00:09<00:00,  3.93it/s, val_loss=0.97] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3: Val F1 = 0.4895\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4 Training:   2%|▏         | 4/175 [00:01<00:58,  2.93it/s, loss=0.0202] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 4 Training: 100%|██████████| 175/175 [01:05<00:00,  2.66it/s, loss=0.897]\nEpoch 4 Validation: 100%|██████████| 38/38 [00:09<00:00,  3.96it/s, val_loss=0.87] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4: Val F1 = 0.5877\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5 Training:  96%|█████████▌| 168/175 [01:03<00:02,  2.96it/s, loss=0.795]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 5 Training: 100%|██████████| 175/175 [01:05<00:00,  2.66it/s, loss=0.827]\nEpoch 5 Validation: 100%|██████████| 38/38 [00:09<00:00,  3.90it/s, val_loss=0.86] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5: Val F1 = 0.5750\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6 Training:  33%|███▎      | 57/175 [00:21<00:44,  2.63it/s, loss=0.249] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 6 Training: 100%|██████████| 175/175 [01:03<00:00,  2.74it/s, loss=0.78] \nEpoch 6 Validation: 100%|██████████| 38/38 [00:09<00:00,  3.94it/s, val_loss=0.937]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 6: Val F1 = 0.5857\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7 Training:  95%|█████████▍| 166/175 [01:00<00:03,  2.86it/s, loss=0.724]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 7 Training: 100%|██████████| 175/175 [01:04<00:00,  2.72it/s, loss=0.762]\nEpoch 7 Validation: 100%|██████████| 38/38 [00:09<00:00,  4.00it/s, val_loss=0.89] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 7: Val F1 = 0.5982\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 8 Training:  14%|█▎        | 24/175 [00:08<00:55,  2.70it/s, loss=0.0842]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 8 Training: 100%|██████████| 175/175 [01:04<00:00,  2.73it/s, loss=0.693]\nEpoch 8 Validation: 100%|██████████| 38/38 [00:09<00:00,  3.89it/s, val_loss=1.04] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 8: Val F1 = 0.5710\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 9 Training:  50%|████▉     | 87/175 [00:32<00:33,  2.62it/s, loss=0.324] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 9 Training: 100%|██████████| 175/175 [01:05<00:00,  2.68it/s, loss=0.651]\nEpoch 9 Validation: 100%|██████████| 38/38 [00:09<00:00,  3.87it/s, val_loss=0.963]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 9: Val F1 = 0.5623\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 10 Training:  41%|████      | 71/175 [00:26<00:41,  2.53it/s, loss=0.226] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 10 Training: 100%|██████████| 175/175 [01:06<00:00,  2.64it/s, loss=0.602]\nEpoch 10 Validation: 100%|██████████| 38/38 [00:09<00:00,  3.81it/s, val_loss=0.937]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 10: Val F1 = 0.6105\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Validation Evaluation: 100%|██████████| 38/38 [00:09<00:00,  3.87it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n📊 Validation Classification Report for VGG16\n                              precision    recall  f1-score   support\n\n            non-misogynistic       0.64      0.72      0.68       207\nstereotype & objectification       0.62      0.66      0.64       239\n           violence or abuse       0.56      0.42      0.48       154\n\n                    accuracy                           0.62       600\n                   macro avg       0.61      0.60      0.60       600\n                weighted avg       0.61      0.62      0.61       600\n\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Test Evaluation: 100%|██████████| 38/38 [00:10<00:00,  3.58it/s]\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n📊 Test Classification Report for VGG16\n                              precision    recall  f1-score   support\n\n            non-misogynistic       0.60      0.75      0.67       207\nstereotype & objectification       0.56      0.54      0.55       239\n           violence or abuse       0.49      0.35      0.41       155\n\n                    accuracy                           0.56       601\n                   macro avg       0.55      0.55      0.54       601\n                weighted avg       0.55      0.56      0.55       601\n\n🧹 Cleared memory after VGG16\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n100%|██████████| 548M/548M [00:03<00:00, 153MB/s]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n🔄 Training VGG19...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 1 Training:  93%|█████████▎| 163/175 [01:02<00:04,  2.70it/s, loss=0.962]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 1 Training: 100%|██████████| 175/175 [01:06<00:00,  2.62it/s, loss=1.03] \nEpoch 1 Validation: 100%|██████████| 38/38 [00:10<00:00,  3.79it/s, val_loss=1.01] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1: Val F1 = 0.4497\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2 Training:  91%|█████████ | 159/175 [01:01<00:06,  2.58it/s, loss=0.884]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 2 Training: 100%|██████████| 175/175 [01:07<00:00,  2.59it/s, loss=0.969]\nEpoch 2 Validation: 100%|██████████| 38/38 [00:09<00:00,  3.89it/s, val_loss=0.999]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2: Val F1 = 0.4179\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3 Training:  53%|█████▎    | 92/175 [00:36<00:32,  2.54it/s, loss=0.496] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 3 Training: 100%|██████████| 175/175 [01:09<00:00,  2.53it/s, loss=0.935]\nEpoch 3 Validation: 100%|██████████| 38/38 [00:09<00:00,  3.82it/s, val_loss=0.953]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3: Val F1 = 0.5124\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4 Training:  11%|█▏        | 20/175 [00:07<01:04,  2.39it/s, loss=0.101] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 4 Training: 100%|██████████| 175/175 [01:06<00:00,  2.62it/s, loss=0.896]\nEpoch 4 Validation: 100%|██████████| 38/38 [00:09<00:00,  3.92it/s, val_loss=0.968]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4: Val F1 = 0.5147\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5 Training:  92%|█████████▏| 161/175 [01:01<00:05,  2.69it/s, loss=0.771]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 5 Training: 100%|██████████| 175/175 [01:06<00:00,  2.62it/s, loss=0.843]\nEpoch 5 Validation: 100%|██████████| 38/38 [00:09<00:00,  3.86it/s, val_loss=0.91] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5: Val F1 = 0.5670\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6 Training:  79%|███████▉  | 139/175 [00:53<00:13,  2.66it/s, loss=0.628]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 6 Training: 100%|██████████| 175/175 [01:07<00:00,  2.59it/s, loss=0.798]\nEpoch 6 Validation: 100%|██████████| 38/38 [00:09<00:00,  3.83it/s, val_loss=0.881]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 6: Val F1 = 0.5608\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7 Training:  93%|█████████▎| 162/175 [01:04<00:04,  2.68it/s, loss=0.693]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 7 Training: 100%|██████████| 175/175 [01:09<00:00,  2.53it/s, loss=0.746]\nEpoch 7 Validation: 100%|██████████| 38/38 [00:10<00:00,  3.72it/s, val_loss=0.94] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 7: Val F1 = 0.5635\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 8 Training:  11%|█▏        | 20/175 [00:07<00:59,  2.61it/s, loss=0.0819]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 8 Training: 100%|██████████| 175/175 [01:07<00:00,  2.57it/s, loss=0.714]\nEpoch 8 Validation: 100%|██████████| 38/38 [00:09<00:00,  3.92it/s, val_loss=0.926]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 8: Val F1 = 0.5643\nEarly stopping at epoch 8\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Validation Evaluation: 100%|██████████| 38/38 [00:09<00:00,  3.91it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n📊 Validation Classification Report for VGG19\n                              precision    recall  f1-score   support\n\n            non-misogynistic       0.64      0.69      0.66       207\nstereotype & objectification       0.58      0.64      0.61       239\n           violence or abuse       0.44      0.33      0.38       154\n\n                    accuracy                           0.57       600\n                   macro avg       0.55      0.55      0.55       600\n                weighted avg       0.56      0.57      0.57       600\n\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Test Evaluation: 100%|██████████| 38/38 [00:10<00:00,  3.49it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n📊 Test Classification Report for VGG19\n                              precision    recall  f1-score   support\n\n            non-misogynistic       0.60      0.71      0.65       207\nstereotype & objectification       0.55      0.59      0.57       239\n           violence or abuse       0.45      0.28      0.35       155\n\n                    accuracy                           0.55       601\n                   macro avg       0.53      0.53      0.52       601\n                weighted avg       0.54      0.55      0.54       601\n\n🧹 Cleared memory after VGG19\n\n📊 Model Comparison (Test Metrics)\nCustomCNN: Accuracy = 0.5191, F1 = 0.5092\nResNet50: Accuracy = 0.6223, F1 = 0.6140\nVGG16: Accuracy = 0.5624, F1 = 0.5525\nVGG19: Accuracy = 0.5541, F1 = 0.5409\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}