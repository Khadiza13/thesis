{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 12259390,
          "sourceType": "datasetVersion",
          "datasetId": 7725149
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Texual_only_thesis",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "OjxNeAKMZi6y"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "khadiza13_less_data_path = kagglehub.dataset_download('khadiza13/less-data')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "VTbB-2bAZi63"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T07:23:27.958083Z",
          "iopub.execute_input": "2025-06-24T07:23:27.958254Z",
          "iopub.status.idle": "2025-06-24T07:23:29.429605Z",
          "shell.execute_reply.started": "2025-06-24T07:23:27.958238Z",
          "shell.execute_reply": "2025-06-24T07:23:29.42889Z"
        },
        "id": "OeOBn2naZi64"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/kaggle/input/less-data/changed_ds/new_ds_small.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T07:23:44.538957Z",
          "iopub.execute_input": "2025-06-24T07:23:44.539208Z",
          "iopub.status.idle": "2025-06-24T07:23:44.618603Z",
          "shell.execute_reply.started": "2025-06-24T07:23:44.539187Z",
          "shell.execute_reply": "2025-06-24T07:23:44.617897Z"
        },
        "id": "UPnqdtPcZi65",
        "outputId": "78c71a25-1698-43c2-d7b5-0db72e295058"
      },
      "outputs": [
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "      image                                               text  \\\n0  2001.jpg  আচ্ছা ভাই।\\n মেয়েদের থেকে দূরে থাকবা মেয়ের পাল...   \n1  2002.jpg  কিসের foodpanda যেখানে আমার সাদিয়া রে অর্ডার ক...   \n2  2003.jpg  উপস্থিত sir\\n Yes sir\\n Present Teacher\\n [লাব...   \n3  2004.png  আমি হিজাব চাইনি\\n তারপর আমার স্বামী আমাকে বোঝালেন   \n4  2005.jpg  বাঙ্গু (beta, omega, theta, delta) male\\nসুন্দ...   \n\n                          label  \n0  stereotype & objectification  \n1  stereotype & objectification  \n2  stereotype & objectification  \n3             violence or abuse  \n4              non-misogynistic  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2001.jpg</td>\n      <td>আচ্ছা ভাই।\\n মেয়েদের থেকে দূরে থাকবা মেয়ের পাল...</td>\n      <td>stereotype &amp; objectification</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2002.jpg</td>\n      <td>কিসের foodpanda যেখানে আমার সাদিয়া রে অর্ডার ক...</td>\n      <td>stereotype &amp; objectification</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003.jpg</td>\n      <td>উপস্থিত sir\\n Yes sir\\n Present Teacher\\n [লাব...</td>\n      <td>stereotype &amp; objectification</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2004.png</td>\n      <td>আমি হিজাব চাইনি\\n তারপর আমার স্বামী আমাকে বোঝালেন</td>\n      <td>violence or abuse</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2005.jpg</td>\n      <td>বাঙ্গু (beta, omega, theta, delta) male\\nসুন্দ...</td>\n      <td>non-misogynistic</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = ['image_name', 'text', 'label']\n",
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T07:23:47.885448Z",
          "iopub.execute_input": "2025-06-24T07:23:47.886074Z",
          "iopub.status.idle": "2025-06-24T07:23:47.894646Z",
          "shell.execute_reply.started": "2025-06-24T07:23:47.886049Z",
          "shell.execute_reply": "2025-06-24T07:23:47.893988Z"
        },
        "id": "g1niuHvaZi66",
        "outputId": "75b2f43b-1839-480f-87c4-4e14bf264336"
      },
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "  image_name                                               text  \\\n0   2001.jpg  আচ্ছা ভাই।\\n মেয়েদের থেকে দূরে থাকবা মেয়ের পাল...   \n1   2002.jpg  কিসের foodpanda যেখানে আমার সাদিয়া রে অর্ডার ক...   \n2   2003.jpg  উপস্থিত sir\\n Yes sir\\n Present Teacher\\n [লাব...   \n3   2004.png  আমি হিজাব চাইনি\\n তারপর আমার স্বামী আমাকে বোঝালেন   \n4   2005.jpg  বাঙ্গু (beta, omega, theta, delta) male\\nসুন্দ...   \n\n                          label  \n0  stereotype & objectification  \n1  stereotype & objectification  \n2  stereotype & objectification  \n3             violence or abuse  \n4              non-misogynistic  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2001.jpg</td>\n      <td>আচ্ছা ভাই।\\n মেয়েদের থেকে দূরে থাকবা মেয়ের পাল...</td>\n      <td>stereotype &amp; objectification</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2002.jpg</td>\n      <td>কিসের foodpanda যেখানে আমার সাদিয়া রে অর্ডার ক...</td>\n      <td>stereotype &amp; objectification</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003.jpg</td>\n      <td>উপস্থিত sir\\n Yes sir\\n Present Teacher\\n [লাব...</td>\n      <td>stereotype &amp; objectification</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2004.png</td>\n      <td>আমি হিজাব চাইনি\\n তারপর আমার স্বামী আমাকে বোঝালেন</td>\n      <td>violence or abuse</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2005.jpg</td>\n      <td>বাঙ্গু (beta, omega, theta, delta) male\\nসুন্দ...</td>\n      <td>non-misogynistic</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers.trainer_callback import EarlyStoppingCallback\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/kaggle/input/less-data/changed_ds/new_ds_small.csv\")\n",
        "assert not df.empty, \"Dataset is empty\"\n",
        "assert df['text'].notnull().all(), \"Missing values in 'text' column\"\n",
        "assert df['label'].notnull().all(), \"Missing values in 'label' column\"\n",
        "print(\"✅ Dataset loaded. Sample:\")\n",
        "print(df.head())\n",
        "\n",
        "# Clean text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[\\n“”\\\"]\", \"\", text)\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation.replace(\"!\", \"\").replace(\"?\", \"\")))\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text.lower()\n",
        "\n",
        "df[\"text\"] = df[\"text\"].astype(str).apply(clean_text)\n",
        "\n",
        "# Encode labels\n",
        "label2id = {label: idx for idx, label in enumerate(df[\"label\"].unique())}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "df[\"label_id\"] = df[\"label\"].map(label2id)\n",
        "print(f\"✅ Unique labels: {label2id}\")\n",
        "print(f\"Label distribution: {df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "# Split data into train (70%), validation (15%), test (15%)\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df[\"text\"].tolist(), df[\"label_id\"].tolist(), test_size=0.3, random_state=42, stratify=df[\"label_id\"]\n",
        ")\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
        ")\n",
        "print(f\"✅ Dataset split: Train: {len(train_texts)}, Val: {len(val_texts)}, Test: {len(test_texts)}\")\n",
        "\n",
        "\n",
        "# Model checkpoints\n",
        "model_names = {\n",
        "    \"mBERT\": \"bert-base-multilingual-cased\"\n",
        "}\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} | {\"labels\": torch.tensor(self.labels[idx])}\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    accuracy = (predictions == labels).mean()\n",
        "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
        "\n",
        "def run_model(model_name, model_checkpoint):\n",
        "    print(f\"\\n🔄 Loading {model_name}...\", flush=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "    # Optimize max_length\n",
        "    tokenized_lengths = [len(tokenizer.encode(text, add_special_tokens=True)) for text in train_texts]\n",
        "    max_length = min(int(np.percentile(tokenized_lengths, 95)), 512)\n",
        "    print(f\"Using max_length: {max_length}\")\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=len(label2id))\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    print(\"🔠 Tokenizing...\", flush=True)\n",
        "    train_enc = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
        "    val_enc = tokenizer(val_texts, truncation=True, padding=True, max_length=max_length)\n",
        "    test_enc = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "    train_dataset = TextDataset(train_enc, train_labels)\n",
        "    val_dataset = TextDataset(val_enc, val_labels)\n",
        "    test_dataset = TextDataset(test_enc, test_labels)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{model_name}\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        num_train_epochs=5,  # Suitable for 4000 samples\n",
        "        per_device_train_batch_size=16,  # Increased for faster training\n",
        "        per_device_eval_batch_size=16,\n",
        "        learning_rate=2e-5,\n",
        "        logging_dir=f\"./logs/{model_name}\",\n",
        "        logging_steps=50,  # Adjusted for 2800 training samples\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",  # Use F1 for balanced performance\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=1,\n",
        "        report_to=\"none\",\n",
        "        disable_tqdm=False\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(\n",
        "            early_stopping_patience=2,\n",
        "            early_stopping_threshold=0.01\n",
        "        )]\n",
        "    )\n",
        "\n",
        "    print(f\"🚀 Training {model_name}...\", flush=True)\n",
        "    trainer.train()\n",
        "\n",
        "    print(f\"🔍 Evaluating {model_name} on validation set...\", flush=True)\n",
        "    predictions = trainer.predict(val_dataset)\n",
        "    preds = np.argmax(predictions.predictions, axis=1)\n",
        "    unique_labels = sorted(set(val_labels))\n",
        "    target_names = [id2label[i] for i in unique_labels]\n",
        "    print(f\"\\n📊 Validation Classification Report for {model_name}\")\n",
        "    print(classification_report(val_labels, preds, labels=unique_labels, target_names=target_names))\n",
        "\n",
        "    print(f\"🔍 Evaluating {model_name} on test set...\", flush=True)\n",
        "    predictions = trainer.predict(test_dataset)\n",
        "    preds = np.argmax(predictions.predictions, axis=1)\n",
        "    unique_labels = sorted(set(test_labels))\n",
        "    target_names = [id2label[i] for i in unique_labels]\n",
        "    print(f\"\\n📊 Test Classification Report for {model_name}\")\n",
        "    print(classification_report(test_labels, preds, labels=unique_labels, target_names=target_names))\n",
        "\n",
        "    # Save the final model\n",
        "    trainer.save_model(f\"./models/{model_name}\")\n",
        "\n",
        "# Run each model\n",
        "for name, checkpoint in model_names.items():\n",
        "    try:\n",
        "        run_model(name, checkpoint)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Skipping {name} due to error: {e}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T07:34:30.128186Z",
          "iopub.execute_input": "2025-06-24T07:34:30.128732Z",
          "iopub.status.idle": "2025-06-24T07:36:56.841116Z",
          "shell.execute_reply.started": "2025-06-24T07:34:30.128699Z",
          "shell.execute_reply": "2025-06-24T07:36:56.840551Z"
        },
        "id": "lxezPh85Zi67",
        "outputId": "e8829542-feef-4923-da57-beda1d8a0e5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Dataset loaded. Sample:\n      image                                               text  \\\n0  2001.jpg  আচ্ছা ভাই।\\n মেয়েদের থেকে দূরে থাকবা মেয়ের পাল...   \n1  2002.jpg  কিসের foodpanda যেখানে আমার সাদিয়া রে অর্ডার ক...   \n2  2003.jpg  উপস্থিত sir\\n Yes sir\\n Present Teacher\\n [লাব...   \n3  2004.png  আমি হিজাব চাইনি\\n তারপর আমার স্বামী আমাকে বোঝালেন   \n4  2005.jpg  বাঙ্গু (beta, omega, theta, delta) male\\nসুন্দ...   \n\n                          label  \n0  stereotype & objectification  \n1  stereotype & objectification  \n2  stereotype & objectification  \n3             violence or abuse  \n4              non-misogynistic  \n✅ Unique labels: {'stereotype & objectification': 0, 'violence or abuse': 1, 'non-misogynistic': 2}\nLabel distribution: {'stereotype & objectification': 1591, 'non-misogynistic': 1380, 'violence or abuse': 1030}\n✅ Dataset split: Train: 2800, Val: 600, Test: 601\n\n🔄 Loading mBERT...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Using max_length: 89\nUsing device: cuda\n🔠 Tokenizing...\n🚀 Training mBERT...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_35/229459967.py:108: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='700' max='875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [700/875 02:19 < 00:34, 5.00 it/s, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.965000</td>\n      <td>0.882353</td>\n      <td>0.576667</td>\n      <td>0.565279</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.752100</td>\n      <td>0.750772</td>\n      <td>0.688333</td>\n      <td>0.685733</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.580100</td>\n      <td>0.895426</td>\n      <td>0.655000</td>\n      <td>0.648574</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.409400</td>\n      <td>0.883383</td>\n      <td>0.681667</td>\n      <td>0.678177</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "🔍 Evaluating mBERT on validation set...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n📊 Validation Classification Report for mBERT\n                              precision    recall  f1-score   support\n\nstereotype & objectification       0.62      0.82      0.70       239\n           violence or abuse       0.70      0.53      0.60       154\n            non-misogynistic       0.82      0.65      0.73       207\n\n                    accuracy                           0.69       600\n                   macro avg       0.71      0.67      0.68       600\n                weighted avg       0.71      0.69      0.69       600\n\n🔍 Evaluating mBERT on test set...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n📊 Test Classification Report for mBERT\n                              precision    recall  f1-score   support\n\nstereotype & objectification       0.56      0.75      0.64       239\n           violence or abuse       0.60      0.49      0.54       155\n            non-misogynistic       0.78      0.58      0.67       207\n\n                    accuracy                           0.62       601\n                   macro avg       0.65      0.61      0.62       601\n                weighted avg       0.65      0.62      0.62       601\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers.trainer_callback import EarlyStoppingCallback\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    random.seed(seed)\n",
        "set_seed(42)\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/kaggle/input/less-data/changed_ds/new_ds_small.csv\")\n",
        "assert not df.empty, \"Dataset is empty\"\n",
        "assert len(df) == 4001, f\"Expected 4000 samples, got {len(df)}\"\n",
        "assert df['text'].notnull().all(), \"Missing values in 'text' column\"\n",
        "assert df['label'].notnull().all(), \"Missing values in 'label' column\"\n",
        "print(\"✅ Dataset loaded. Sample:\")\n",
        "print(df.head())\n",
        "\n",
        "# Clean text for Banglish\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[\\n“”\\\"]\", \"\", text)\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation.replace(\"!\", \"\").replace(\"?\", \"\")))\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)  # Normalize repeated characters (e.g., \"sooo\" -> \"soo\")\n",
        "    # text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)  # Uncomment to remove emojis\n",
        "    return text\n",
        "\n",
        "df[\"text\"] = df[\"text\"].astype(str).apply(clean_text)\n",
        "\n",
        "# Encode labels\n",
        "label2id = {label: idx for idx, label in enumerate(df[\"label\"].unique())}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "df[\"label_id\"] = df[\"label\"].map(label2id)\n",
        "print(f\"✅ Unique labels: {label2id}\")\n",
        "print(f\"Label distribution: {df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "# Split data into train (70%), validation (15%), test (15%)\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    df[\"text\"].tolist(), df[\"label_id\"].tolist(), test_size=0.3, random_state=42, stratify=df[\"label_id\"]\n",
        ")\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
        ")\n",
        "print(f\"✅ Dataset split: Train: {len(train_texts)}, Val: {len(val_texts)}, Test: {len(test_texts)}\")\n",
        "\n",
        "# Model checkpoints\n",
        "model_names = {\n",
        "    \"BanglishBERT\": \"csebuetnlp/banglishbert\",\n",
        "    \"BanglaBERT\": \"csebuetnlp/banglabert\",\n",
        "    \"MuRILBERT\": \"google/muril-base-cased\",\n",
        "    \"XLM-RoBERTa\": \"xlm-roberta-base\",\n",
        "    \"DistilBERT\": \"distilbert-base-multilingual-cased\"\n",
        "}\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} | {\"labels\": torch.tensor(self.labels[idx])}\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    accuracy = (predictions == labels).mean()\n",
        "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
        "\n",
        "def run_model(model_name, model_checkpoint):\n",
        "    print(f\"\\n🔄 Loading {model_name}...\", flush=True)\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "    # Optimize max_length\n",
        "    tokenized_lengths = [len(tokenizer.encode(text, add_special_tokens=True)) for text in train_texts]\n",
        "    max_length = min(int(np.percentile(tokenized_lengths, 95)), 512)\n",
        "    print(f\"Using max_length: {max_length}\")\n",
        "\n",
        "    # Load model for classification\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_checkpoint, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
        "    )\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    print(\"🔠 Tokenizing...\", flush=True)\n",
        "    train_enc = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
        "    val_enc = tokenizer(val_texts, truncation=True, padding=True, max_length=max_length)\n",
        "    test_enc = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "    train_dataset = TextDataset(train_enc, train_labels)\n",
        "    val_dataset = TextDataset(val_enc, val_labels)\n",
        "    test_dataset = TextDataset(test_enc, test_labels)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{model_name}\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        num_train_epochs=10,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        gradient_accumulation_steps=2,\n",
        "        learning_rate=2e-5,\n",
        "        logging_dir=f\"./logs/{model_name}\",\n",
        "        logging_steps=50,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=1,\n",
        "        report_to=\"none\",\n",
        "        disable_tqdm=False,\n",
        "        fp16=True  # Mixed precision for faster training\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(\n",
        "            early_stopping_patience=2,\n",
        "            early_stopping_threshold=0.01\n",
        "        )]\n",
        "    )\n",
        "\n",
        "    print(f\"🚀 Training {model_name}...\", flush=True)\n",
        "    trainer.train()\n",
        "\n",
        "    print(f\"🔍 Evaluating {model_name} on validation set...\", flush=True)\n",
        "    val_predictions = trainer.predict(val_dataset)\n",
        "    val_preds = np.argmax(val_predictions.predictions, axis=1)\n",
        "    unique_labels = sorted(set(val_labels))\n",
        "    target_names = [id2label[i] for i in unique_labels]\n",
        "    print(f\"\\n📊 Validation Classification Report for {model_name}\")\n",
        "    val_report = classification_report(val_labels, val_preds, labels=unique_labels, target_names=target_names)\n",
        "    print(val_report)\n",
        "\n",
        "    print(f\"🔍 Evaluating {model_name} on test set...\", flush=True)\n",
        "    test_predictions = trainer.predict(test_dataset)\n",
        "    test_preds = np.argmax(test_predictions.predictions, axis=1)\n",
        "    print(f\"\\n📊 Test Classification Report for {model_name}\")\n",
        "    test_report = classification_report(test_labels, test_preds, labels=unique_labels, target_names=target_names)\n",
        "    print(test_report)\n",
        "\n",
        "    # Save outputs\n",
        "    os.makedirs(f\"./predictions/{model_name}\", exist_ok=True)\n",
        "    np.save(f\"./predictions/{model_name}/val_preds.npy\", val_predictions.predictions)\n",
        "    np.save(f\"./predictions/{model_name}/test_preds.npy\", test_predictions.predictions)\n",
        "    trainer.save_model(f\"./results/{model_name}\")\n",
        "\n",
        "    # Store results\n",
        "    results[model_name] = {\n",
        "        \"val_report\": classification_report(val_labels, val_preds, labels=unique_labels, target_names=target_names, output_dict=True),\n",
        "        \"test_report\": classification_report(test_labels, test_preds, labels=unique_labels, target_names=target_names, output_dict=True)\n",
        "    }\n",
        "\n",
        "    # Clear memory\n",
        "    del model\n",
        "    del trainer\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"🧹 Cleared memory after {model_name}\")\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# Run models\n",
        "for name, checkpoint in model_names.items():\n",
        "    try:\n",
        "        run_model(name, checkpoint)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Skipping {name} due to error: {e}\")\n",
        "\n",
        "# Compare results\n",
        "print(\"\\n📊 Model Comparison (Test F1)\")\n",
        "for model_name, result in results.items():\n",
        "    test_f1 = result[\"test_report\"][\"weighted avg\"][\"f1-score\"]\n",
        "    print(f\"{model_name}: {test_f1:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T07:49:47.529763Z",
          "iopub.execute_input": "2025-06-24T07:49:47.53032Z",
          "iopub.status.idle": "2025-06-24T08:03:28.037325Z",
          "shell.execute_reply.started": "2025-06-24T07:49:47.530297Z",
          "shell.execute_reply": "2025-06-24T08:03:28.036548Z"
        },
        "id": "GSYdF8gCZi69",
        "outputId": "4851ba3e-0228-48fb-f1dc-5d9d990bdc61",
        "colab": {
          "referenced_widgets": [
            "51ad11daf44e478f8f3b6e0371816579",
            "e55d023296714b898cde0066a8098eb2",
            "c43c1e7f630342a8aec9e4edad12960d",
            "acfa977a1a414e26a66f7d7a20a769e8",
            "0be01f3f2f1e4b6990a8fa4ab8c3c12f",
            "8d054ea7dba140f4b6b1b2ca5571dc95",
            "c89c9cfa38c840ada949df79036d0b4f",
            "e652a867529b475ead9827a3298d32ed",
            "9702d2256c1d4527886d822b23d04601",
            "3978dcd76c4e4e4682fa5810bca20a84",
            "168dec8ff5d5454582d693190c720097",
            "770cbc949fec4cccba4cac88d9271932",
            "a4f5a3657b7d4b2abe4f46372d87d206",
            "0e8f1f066a4c4111a64a2faae3d90657",
            "e4846f028aee4481bf20e2b0babd5a3f",
            "1f33d57f31dc41e9a719789ce05e1394",
            "a2e0cfa1994f4d52968db7c92dd1d072",
            "b926354ce6d24b31aebf9e9241305a63",
            "4112dbf4bd584dbbbf31cf368484936e",
            "e1c4e84047c84f87a5051620c21d5c3a",
            "890530a13d8d4d9daeed88dbde4793a8",
            "61b2c7dd8ea145b1ba2f15d908ca6a58"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Dataset loaded. Sample:\n      image                                               text  \\\n0  2001.jpg  আচ্ছা ভাই।\\n মেয়েদের থেকে দূরে থাকবা মেয়ের পাল...   \n1  2002.jpg  কিসের foodpanda যেখানে আমার সাদিয়া রে অর্ডার ক...   \n2  2003.jpg  উপস্থিত sir\\n Yes sir\\n Present Teacher\\n [লাব...   \n3  2004.png  আমি হিজাব চাইনি\\n তারপর আমার স্বামী আমাকে বোঝালেন   \n4  2005.jpg  বাঙ্গু (beta, omega, theta, delta) male\\nসুন্দ...   \n\n                          label  \n0  stereotype & objectification  \n1  stereotype & objectification  \n2  stereotype & objectification  \n3             violence or abuse  \n4              non-misogynistic  \n✅ Unique labels: {'stereotype & objectification': 0, 'violence or abuse': 1, 'non-misogynistic': 2}\nLabel distribution: {'stereotype & objectification': 1591, 'non-misogynistic': 1380, 'violence or abuse': 1030}\n✅ Dataset split: Train: 2800, Val: 600, Test: 601\n\n🔄 Loading BanglishBERT...\nUsing max_length: 48\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglishbert and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Using device: cuda\n🔠 Tokenizing...\n🚀 Training BanglishBERT...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_35/2461144152.py:131: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='875' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 875/1750 02:04 < 02:04, 7.01 it/s, Epoch 5/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.918800</td>\n      <td>0.830626</td>\n      <td>0.606667</td>\n      <td>0.556218</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.663000</td>\n      <td>0.726623</td>\n      <td>0.688333</td>\n      <td>0.683901</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.400900</td>\n      <td>0.783682</td>\n      <td>0.698333</td>\n      <td>0.695961</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.239900</td>\n      <td>1.014558</td>\n      <td>0.693333</td>\n      <td>0.688286</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.111200</td>\n      <td>1.437472</td>\n      <td>0.705000</td>\n      <td>0.694690</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "🔍 Evaluating BanglishBERT on validation set...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n📊 Validation Classification Report for BanglishBERT\n                              precision    recall  f1-score   support\n\nstereotype & objectification       0.68      0.70      0.69       239\n           violence or abuse       0.65      0.56      0.61       154\n            non-misogynistic       0.74      0.79      0.77       207\n\n                    accuracy                           0.70       600\n                   macro avg       0.69      0.69      0.69       600\n                weighted avg       0.70      0.70      0.70       600\n\n🔍 Evaluating BanglishBERT on test set...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n📊 Test Classification Report for BanglishBERT\n                              precision    recall  f1-score   support\n\nstereotype & objectification       0.70      0.74      0.72       239\n           violence or abuse       0.59      0.56      0.57       155\n            non-misogynistic       0.77      0.74      0.75       207\n\n                    accuracy                           0.69       601\n                   macro avg       0.68      0.68      0.68       601\n                weighted avg       0.69      0.69      0.69       601\n\n🧹 Cleared memory after BanglishBERT\n\n🔄 Loading BanglaBERT...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51ad11daf44e478f8f3b6e0371816579"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/586 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e55d023296714b898cde0066a8098eb2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/528k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c43c1e7f630342a8aec9e4edad12960d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acfa977a1a414e26a66f7d7a20a769e8"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Using max_length: 48\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0be01f3f2f1e4b6990a8fa4ab8c3c12f"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Using device: cuda\n🔠 Tokenizing...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d054ea7dba140f4b6b1b2ca5571dc95"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "🚀 Training BanglaBERT...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_35/2461144152.py:131: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='700' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 700/1750 01:37 < 02:26, 7.18 it/s, Epoch 4/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.938300</td>\n      <td>0.850361</td>\n      <td>0.616667</td>\n      <td>0.550693</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.700200</td>\n      <td>0.703674</td>\n      <td>0.718333</td>\n      <td>0.718784</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.517200</td>\n      <td>0.712947</td>\n      <td>0.716667</td>\n      <td>0.718470</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.321800</td>\n      <td>0.831664</td>\n      <td>0.710000</td>\n      <td>0.705050</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "🔍 Evaluating BanglaBERT on validation set...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n📊 Validation Classification Report for BanglaBERT\n                              precision    recall  f1-score   support\n\nstereotype & objectification       0.71      0.70      0.71       239\n           violence or abuse       0.61      0.63      0.62       154\n            non-misogynistic       0.81      0.80      0.80       207\n\n                    accuracy                           0.72       600\n                   macro avg       0.71      0.71      0.71       600\n                weighted avg       0.72      0.72      0.72       600\n\n🔍 Evaluating BanglaBERT on test set...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n📊 Test Classification Report for BanglaBERT\n                              precision    recall  f1-score   support\n\nstereotype & objectification       0.73      0.77      0.75       239\n           violence or abuse       0.60      0.65      0.63       155\n            non-misogynistic       0.85      0.74      0.79       207\n\n                    accuracy                           0.73       601\n                   macro avg       0.73      0.72      0.72       601\n                weighted avg       0.74      0.73      0.73       601\n\n🧹 Cleared memory after BanglaBERT\n\n🔄 Loading MuRILBERT...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c89c9cfa38c840ada949df79036d0b4f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e652a867529b475ead9827a3298d32ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9702d2256c1d4527886d822b23d04601"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/113 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3978dcd76c4e4e4682fa5810bca20a84"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Using max_length: 52\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/953M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "168dec8ff5d5454582d693190c720097"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/953M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "770cbc949fec4cccba4cac88d9271932"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Using device: cuda\n🔠 Tokenizing...\n🚀 Training MuRILBERT...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_35/2461144152.py:131: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1050' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1050/1750 03:18 < 02:12, 5.27 it/s, Epoch 6/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.031000</td>\n      <td>0.971366</td>\n      <td>0.623333</td>\n      <td>0.572578</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.867400</td>\n      <td>0.865668</td>\n      <td>0.673333</td>\n      <td>0.664238</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.745700</td>\n      <td>0.795872</td>\n      <td>0.678333</td>\n      <td>0.668232</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.605900</td>\n      <td>0.765444</td>\n      <td>0.698333</td>\n      <td>0.695734</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.525700</td>\n      <td>0.819008</td>\n      <td>0.676667</td>\n      <td>0.672267</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.403200</td>\n      <td>0.867744</td>\n      <td>0.673333</td>\n      <td>0.670274</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "🔍 Evaluating MuRILBERT on validation set...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n📊 Validation Classification Report for MuRILBERT\n                              precision    recall  f1-score   support\n\nstereotype & objectification       0.64      0.77      0.69       239\n           violence or abuse       0.72      0.53      0.61       154\n            non-misogynistic       0.78      0.75      0.76       207\n\n                    accuracy                           0.70       600\n                   macro avg       0.71      0.68      0.69       600\n                weighted avg       0.71      0.70      0.70       600\n\n🔍 Evaluating MuRILBERT on test set...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n📊 Test Classification Report for MuRILBERT\n                              precision    recall  f1-score   support\n\nstereotype & objectification       0.67      0.85      0.75       239\n           violence or abuse       0.71      0.54      0.61       155\n            non-misogynistic       0.85      0.74      0.79       207\n\n                    accuracy                           0.73       601\n                   macro avg       0.74      0.71      0.72       601\n                weighted avg       0.74      0.73      0.73       601\n\n🧹 Cleared memory after MuRILBERT\n\n🔄 Loading XLM-RoBERTa...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4f5a3657b7d4b2abe4f46372d87d206"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e8f1f066a4c4111a64a2faae3d90657"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4846f028aee4481bf20e2b0babd5a3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f33d57f31dc41e9a719789ce05e1394"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Using max_length: 68\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2e0cfa1994f4d52968db7c92dd1d072"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Using device: cuda\n🔠 Tokenizing...\n🚀 Training XLM-RoBERTa...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_35/2461144152.py:131: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1050' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1050/1750 04:09 < 02:46, 4.21 it/s, Epoch 6/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.961600</td>\n      <td>0.795764</td>\n      <td>0.665000</td>\n      <td>0.645352</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.747900</td>\n      <td>0.771774</td>\n      <td>0.681667</td>\n      <td>0.679551</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.614600</td>\n      <td>0.712961</td>\n      <td>0.708333</td>\n      <td>0.706629</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.493100</td>\n      <td>0.765000</td>\n      <td>0.743333</td>\n      <td>0.740715</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.458600</td>\n      <td>0.917212</td>\n      <td>0.700000</td>\n      <td>0.701871</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.304000</td>\n      <td>0.945769</td>\n      <td>0.723333</td>\n      <td>0.720818</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "🔍 Evaluating XLM-RoBERTa on validation set...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n📊 Validation Classification Report for XLM-RoBERTa\n                              precision    recall  f1-score   support\n\nstereotype & objectification       0.73      0.75      0.74       239\n           violence or abuse       0.72      0.60      0.66       154\n            non-misogynistic       0.77      0.84      0.80       207\n\n                    accuracy                           0.74       600\n                   macro avg       0.74      0.73      0.73       600\n                weighted avg       0.74      0.74      0.74       600\n\n🔍 Evaluating XLM-RoBERTa on test set...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n📊 Test Classification Report for XLM-RoBERTa\n                              precision    recall  f1-score   support\n\nstereotype & objectification       0.75      0.76      0.75       239\n           violence or abuse       0.66      0.61      0.63       155\n            non-misogynistic       0.78      0.82      0.80       207\n\n                    accuracy                           0.74       601\n                   macro avg       0.73      0.73      0.73       601\n                weighted avg       0.74      0.74      0.74       601\n\n🧹 Cleared memory after XLM-RoBERTa\n\n🔄 Loading DistilBERT...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b926354ce6d24b31aebf9e9241305a63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4112dbf4bd584dbbbf31cf368484936e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1c4e84047c84f87a5051620c21d5c3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "890530a13d8d4d9daeed88dbde4793a8"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Using max_length: 89\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61b2c7dd8ea145b1ba2f15d908ca6a58"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Using device: cuda\n🔠 Tokenizing...\n🚀 Training DistilBERT...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_35/2461144152.py:131: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='700' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 700/1750 01:35 < 02:23, 7.30 it/s, Epoch 4/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.989000</td>\n      <td>0.912250</td>\n      <td>0.571667</td>\n      <td>0.561748</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.797800</td>\n      <td>0.782699</td>\n      <td>0.675000</td>\n      <td>0.672421</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.660900</td>\n      <td>0.827154</td>\n      <td>0.628333</td>\n      <td>0.616910</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.472600</td>\n      <td>0.854483</td>\n      <td>0.665000</td>\n      <td>0.660121</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "🔍 Evaluating DistilBERT on validation set...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n📊 Validation Classification Report for DistilBERT\n                              precision    recall  f1-score   support\n\nstereotype & objectification       0.64      0.69      0.66       239\n           violence or abuse       0.65      0.53      0.59       154\n            non-misogynistic       0.73      0.77      0.75       207\n\n                    accuracy                           0.68       600\n                   macro avg       0.67      0.66      0.67       600\n                weighted avg       0.67      0.68      0.67       600\n\n🔍 Evaluating DistilBERT on test set...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n📊 Test Classification Report for DistilBERT\n                              precision    recall  f1-score   support\n\nstereotype & objectification       0.58      0.65      0.62       239\n           violence or abuse       0.58      0.50      0.53       155\n            non-misogynistic       0.69      0.68      0.69       207\n\n                    accuracy                           0.62       601\n                   macro avg       0.62      0.61      0.61       601\n                weighted avg       0.62      0.62      0.62       601\n\n🧹 Cleared memory after DistilBERT\n\n📊 Model Comparison (Test F1)\nBanglishBERT: 0.6916\nBanglaBERT: 0.7310\nMuRILBERT: 0.7271\nXLM-RoBERTa: 0.7388\nDistilBERT: 0.6194\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}