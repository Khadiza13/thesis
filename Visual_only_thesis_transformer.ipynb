{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 12259390,
          "sourceType": "datasetVersion",
          "datasetId": 7725149
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Visual_only_thesis_transformer",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "ZevPd3mtY877"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "khadiza13_less_data_path = kagglehub.dataset_download('khadiza13/less-data')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "n03E6jjLY88A"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T09:04:00.670329Z",
          "iopub.execute_input": "2025-06-24T09:04:00.670485Z",
          "iopub.status.idle": "2025-06-24T09:04:02.432703Z",
          "shell.execute_reply.started": "2025-06-24T09:04:00.670469Z",
          "shell.execute_reply": "2025-06-24T09:04:02.432042Z"
        },
        "id": "z8qsh5NpY88B"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/kaggle/input/less-data/changed_ds/new_ds_small.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T09:05:18.23732Z",
          "iopub.execute_input": "2025-06-24T09:05:18.238068Z",
          "iopub.status.idle": "2025-06-24T09:05:18.319014Z",
          "shell.execute_reply.started": "2025-06-24T09:05:18.238043Z",
          "shell.execute_reply": "2025-06-24T09:05:18.318311Z"
        },
        "id": "sRdIQxjQY88C",
        "outputId": "8d2793c0-f7b5-4ab2-c5ac-3349e3dcbece"
      },
      "outputs": [
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "      image                                               text  \\\n0  2001.jpg  আচ্ছা ভাই।\\n মেয়েদের থেকে দূরে থাকবা মেয়ের পাল...   \n1  2002.jpg  কিসের foodpanda যেখানে আমার সাদিয়া রে অর্ডার ক...   \n2  2003.jpg  উপস্থিত sir\\n Yes sir\\n Present Teacher\\n [লাব...   \n3  2004.png  আমি হিজাব চাইনি\\n তারপর আমার স্বামী আমাকে বোঝালেন   \n4  2005.jpg  বাঙ্গু (beta, omega, theta, delta) male\\nসুন্দ...   \n\n                          label  \n0  stereotype & objectification  \n1  stereotype & objectification  \n2  stereotype & objectification  \n3             violence or abuse  \n4              non-misogynistic  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2001.jpg</td>\n      <td>আচ্ছা ভাই।\\n মেয়েদের থেকে দূরে থাকবা মেয়ের পাল...</td>\n      <td>stereotype &amp; objectification</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2002.jpg</td>\n      <td>কিসের foodpanda যেখানে আমার সাদিয়া রে অর্ডার ক...</td>\n      <td>stereotype &amp; objectification</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003.jpg</td>\n      <td>উপস্থিত sir\\n Yes sir\\n Present Teacher\\n [লাব...</td>\n      <td>stereotype &amp; objectification</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2004.png</td>\n      <td>আমি হিজাব চাইনি\\n তারপর আমার স্বামী আমাকে বোঝালেন</td>\n      <td>violence or abuse</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2005.jpg</td>\n      <td>বাঙ্গু (beta, omega, theta, delta) male\\nসুন্দ...</td>\n      <td>non-misogynistic</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    random.seed(seed)\n",
        "set_seed(42)\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/kaggle/input/less-data/changed_ds/new_ds_small.csv\")\n",
        "assert not df.empty, \"Dataset is empty\"\n",
        "assert len(df) == 4001, f\"Expected 4001 samples, got {len(df)}\"\n",
        "assert df['image'].notnull().all(), \"Missing values in 'image' column\"\n",
        "assert df['label'].notnull().all(), \"Missing values in 'label' column\"\n",
        "print(\"✅ Dataset loaded. Sample:\")\n",
        "print(df.head())\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['label_id'] = label_encoder.fit_transform(df['label'])\n",
        "label2id = {label: idx for idx, label in enumerate(label_encoder.classes_)}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "print(f\"✅ Unique labels: {label2id}\")\n",
        "print(f\"Label distribution: {df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "# Split data into train (70%), validation (15%), test (15%)\n",
        "train_df, temp_df = train_test_split(\n",
        "    df, test_size=0.3, random_state=42, stratify=df['label_id']\n",
        ")\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.5, random_state=42, stratify=temp_df['label_id']\n",
        ")\n",
        "print(f\"✅ Dataset split: Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
        "\n",
        "# Image dataset class\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, processor=None, transform=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.processor = processor\n",
        "        self.transform = transform\n",
        "        self.image_paths = df['image'].values\n",
        "        self.labels = df['label_id'].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.image_paths[idx])\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            image = Image.new('RGB', (224, 224))  # Fallback: blank image\n",
        "        if self.processor:\n",
        "            image = self.processor(image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
        "        elif self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return {'pixel_values' if self.processor else 'image': image, 'labels': label}\n",
        "\n",
        "# Preprocessing for EfficientNet\n",
        "efficientnet_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "efficientnet_val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Model configurations\n",
        "model_configs = {\n",
        "    \"EfficientNet\": {\n",
        "        \"model_fn\": lambda num_classes: models.efficientnet_b0(pretrained=True),\n",
        "        \"modify_fn\": lambda model, num_classes: setattr(model, 'classifier', nn.Linear(model.classifier[1].in_features, num_classes)),\n",
        "        \"transform\": efficientnet_transform,\n",
        "        \"val_test_transform\": efficientnet_val_test_transform,\n",
        "        \"use_processor\": False\n",
        "    },\n",
        "    \"ViT\": {\n",
        "        \"checkpoint\": \"google/vit-base-patch16-224\",\n",
        "        \"use_processor\": True\n",
        "    },\n",
        "    \"SwinTransformer\": {\n",
        "        \"checkpoint\": \"microsoft/swin-base-patch4-window7-224\",\n",
        "        \"use_processor\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "# Training and evaluation function\n",
        "def train_and_evaluate(model_name, model, train_loader, val_loader, test_loader, num_classes, device):\n",
        "    print(f\"\\n🔄 Training {model_name}...\")\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 3\n",
        "    min_delta = 0.01\n",
        "    best_f1 = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    num_epochs = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        # Progress bar for training\n",
        "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\")\n",
        "        for batch in train_bar:\n",
        "            images = batch['image' if model_name == \"EfficientNet\" else 'pixel_values'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            logits = outputs.logits if model_name != \"EfficientNet\" else outputs\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            train_bar.set_postfix({\"loss\": running_loss / len(train_bar)})\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_preds, val_labels = [], []\n",
        "        val_loss = 0.0\n",
        "        # Progress bar for validation\n",
        "        val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\")\n",
        "        with torch.no_grad():\n",
        "            for batch in val_bar:\n",
        "                images = batch['image' if model_name == \"EfficientNet\" else 'pixel_values'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "                outputs = model(images)\n",
        "                logits = outputs.logits if model_name != \"EfficientNet\" else outputs\n",
        "                loss = criterion(logits, labels)\n",
        "                val_loss += loss.item()\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                val_preds.extend(preds.cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "                val_bar.set_postfix({\"val_loss\": val_loss / len(val_bar)})\n",
        "\n",
        "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
        "        print(f\"Epoch {epoch+1}: Val F1 = {val_f1:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_f1 > best_f1 + min_delta:\n",
        "            best_f1 = val_f1\n",
        "            patience_counter = 0\n",
        "            os.makedirs(f\"./models/{model_name}\", exist_ok=True)\n",
        "            torch.save(model.state_dict(), f\"./models/{model_name}/best_model.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(f\"./models/{model_name}/best_model.pt\"))\n",
        "\n",
        "    # Validation evaluation\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    # Progress bar for validation evaluation\n",
        "    val_bar = tqdm(val_loader, desc=\"Validation Evaluation\")\n",
        "    with torch.no_grad():\n",
        "        for batch in val_bar:\n",
        "            images = batch['image' if model_name == \"EfficientNet\" else 'pixel_values'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(images)\n",
        "            logits = outputs.logits if model_name != \"EfficientNet\" else outputs\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(f\"\\n📊 Validation Classification Report for {model_name}\")\n",
        "    val_report = classification_report(val_labels, val_preds, target_names=label_encoder.classes_)\n",
        "    print(val_report)\n",
        "\n",
        "    # Test evaluation\n",
        "    test_preds, test_labels = [], []\n",
        "    # Progress bar for test evaluation\n",
        "    test_bar = tqdm(test_loader, desc=\"Test Evaluation\")\n",
        "    with torch.no_grad():\n",
        "        for batch in test_bar:\n",
        "            images = batch['image' if model_name == \"EfficientNet\" else 'pixel_values'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(images)\n",
        "            logits = outputs.logits if model_name != \"EfficientNet\" else outputs\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            test_preds.extend(preds.cpu().numpy())\n",
        "            test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(f\"\\n📊 Test Classification Report for {model_name}\")\n",
        "    test_report = classification_report(test_labels, test_preds, target_names=label_encoder.classes_)\n",
        "    print(test_report)\n",
        "\n",
        "    # Save results\n",
        "    results[model_name] = {\n",
        "        \"val_report\": classification_report(val_labels, val_preds, target_names=label_encoder.classes_, output_dict=True),\n",
        "        \"test_report\": classification_report(test_labels, test_preds, target_names=label_encoder.classes_, output_dict=True)\n",
        "    }\n",
        "\n",
        "    # Clear memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"🧹 Cleared memory after {model_name}\")\n",
        "\n",
        "# Main execution\n",
        "results = {}\n",
        "model_names = [\"EfficientNet\", \"ViT\", \"SwinTransformer\"]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create datasets and loaders\n",
        "img_dir = \"/kaggle/input/less-data/changed_ds/img\"\n",
        "for model_name in model_names:\n",
        "    config = model_configs[model_name]\n",
        "    if config[\"use_processor\"]:\n",
        "        processor = AutoImageProcessor.from_pretrained(config[\"checkpoint\"])\n",
        "        train_dataset = ImageDataset(train_df, img_dir, processor=processor)\n",
        "        val_dataset = ImageDataset(val_df, img_dir, processor=processor)\n",
        "        test_dataset = ImageDataset(test_df, img_dir, processor=processor)\n",
        "    else:\n",
        "        train_dataset = ImageDataset(train_df, img_dir, transform=config[\"transform\"])\n",
        "        val_dataset = ImageDataset(val_df, img_dir, transform=config[\"val_test_transform\"])\n",
        "        test_dataset = ImageDataset(test_df, img_dir, transform=config[\"val_test_transform\"])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    # Initialize model\n",
        "    if model_name == \"EfficientNet\":\n",
        "        model = config[\"model_fn\"](len(label2id))\n",
        "        config[\"modify_fn\"](model, len(label2id))\n",
        "    else:\n",
        "        model = AutoModelForImageClassification.from_pretrained(\n",
        "            config[\"checkpoint\"],\n",
        "            num_labels=len(label2id),\n",
        "            id2label=id2label,\n",
        "            label2id=label2id\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        train_and_evaluate(model_name, model, train_loader, val_loader, test_loader, len(label2id), device)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Skipping {model_name} due to error: {e}\")\n",
        "\n",
        "# Compare results\n",
        "print(\"\\n📊 Model Comparison (Test Metrics)\")\n",
        "for model_name, result in results.items():\n",
        "    test_acc = result[\"test_report\"][\"accuracy\"]\n",
        "    test_f1 = result[\"test_report\"][\"weighted avg\"][\"f1-score\"]\n",
        "    print(f\"{model_name}: Accuracy = {test_acc:.4f}, F1 = {test_f1:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T09:07:25.931016Z",
          "iopub.execute_input": "2025-06-24T09:07:25.931657Z",
          "iopub.status.idle": "2025-06-24T09:16:07.043914Z",
          "shell.execute_reply.started": "2025-06-24T09:07:25.931627Z",
          "shell.execute_reply": "2025-06-24T09:16:07.042857Z"
        },
        "id": "E5ccn5gOY88D",
        "outputId": "56c37187-2ba2-4aed-e61b-93214149b4ae",
        "colab": {
          "referenced_widgets": [
            "22da3d67107e4f2cbc22a9701abf03dc",
            "bce1e700ee514d94a0bbbf05e5a084d1",
            "ee622fb4eaab476ebcffe09de7f3cda4"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-06-24 09:07:39.717975: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750756059.913615      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750756059.974633      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Dataset loaded. Sample:\n      image                                               text  \\\n0  2001.jpg  আচ্ছা ভাই।\\n মেয়েদের থেকে দূরে থাকবা মেয়ের পাল...   \n1  2002.jpg  কিসের foodpanda যেখানে আমার সাদিয়া রে অর্ডার ক...   \n2  2003.jpg  উপস্থিত sir\\n Yes sir\\n Present Teacher\\n [লাব...   \n3  2004.png  আমি হিজাব চাইনি\\n তারপর আমার স্বামী আমাকে বোঝালেন   \n4  2005.jpg  বাঙ্গু (beta, omega, theta, delta) male\\nসুন্দ...   \n\n                          label  \n0  stereotype & objectification  \n1  stereotype & objectification  \n2  stereotype & objectification  \n3             violence or abuse  \n4              non-misogynistic  \n✅ Unique labels: {'non-misogynistic': 0, 'stereotype & objectification': 1, 'violence or abuse': 2}\nLabel distribution: {'stereotype & objectification': 1591, 'non-misogynistic': 1380, 'violence or abuse': 1030}\n✅ Dataset split: Train: 2800, Val: 600, Test: 601\nUsing device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 20.5M/20.5M [00:00<00:00, 179MB/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n🔄 Training EfficientNet...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 1 Training:  66%|██████▌   | 115/175 [00:48<00:26,  2.24it/s, loss=0.668]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 1 Training: 100%|██████████| 175/175 [01:12<00:00,  2.41it/s, loss=0.998]\nEpoch 1 Validation: 100%|██████████| 38/38 [00:12<00:00,  2.94it/s, val_loss=0.907]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1: Val F1 = 0.5746\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2 Training:  63%|██████▎   | 110/175 [00:33<00:19,  3.39it/s, loss=0.522]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 2 Training: 100%|██████████| 175/175 [00:54<00:00,  3.23it/s, loss=0.834]\nEpoch 2 Validation: 100%|██████████| 38/38 [00:09<00:00,  4.16it/s, val_loss=0.908]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2: Val F1 = 0.5821\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3 Training:  11%|█▏        | 20/175 [00:06<00:45,  3.38it/s, loss=0.086] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 3 Training: 100%|██████████| 175/175 [00:54<00:00,  3.22it/s, loss=0.714]\nEpoch 3 Validation: 100%|██████████| 38/38 [00:09<00:00,  4.07it/s, val_loss=0.902]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3: Val F1 = 0.5808\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4 Training:  25%|██▌       | 44/175 [00:13<00:42,  3.11it/s, loss=0.156] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 4 Training: 100%|██████████| 175/175 [00:54<00:00,  3.21it/s, loss=0.587]\nEpoch 4 Validation: 100%|██████████| 38/38 [00:09<00:00,  4.19it/s, val_loss=0.938]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4: Val F1 = 0.6126\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5 Training:  30%|███       | 53/175 [00:15<00:35,  3.44it/s, loss=0.136] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 5 Training: 100%|██████████| 175/175 [00:54<00:00,  3.21it/s, loss=0.477]\nEpoch 5 Validation: 100%|██████████| 38/38 [00:09<00:00,  4.16it/s, val_loss=0.982]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5: Val F1 = 0.6043\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6 Training:  78%|███████▊  | 136/175 [00:41<00:11,  3.45it/s, loss=0.304]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 6 Training: 100%|██████████| 175/175 [00:54<00:00,  3.24it/s, loss=0.399]\nEpoch 6 Validation: 100%|██████████| 38/38 [00:09<00:00,  4.19it/s, val_loss=1.05] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 6: Val F1 = 0.6042\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7 Training:  38%|███▊      | 67/175 [00:20<00:32,  3.36it/s, loss=0.112]  /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 7 Training: 100%|██████████| 175/175 [00:54<00:00,  3.22it/s, loss=0.288]\nEpoch 7 Validation: 100%|██████████| 38/38 [00:09<00:00,  4.16it/s, val_loss=1.13] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 7: Val F1 = 0.6116\nEarly stopping at epoch 7\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Validation Evaluation: 100%|██████████| 38/38 [00:08<00:00,  4.25it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n📊 Validation Classification Report for EfficientNet\n                              precision    recall  f1-score   support\n\n            non-misogynistic       0.72      0.68      0.70       207\nstereotype & objectification       0.62      0.65      0.64       239\n           violence or abuse       0.46      0.47      0.46       154\n\n                    accuracy                           0.61       600\n                   macro avg       0.60      0.60      0.60       600\n                weighted avg       0.61      0.61      0.61       600\n\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Test Evaluation: 100%|██████████| 38/38 [00:12<00:00,  3.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n📊 Test Classification Report for EfficientNet\n                              precision    recall  f1-score   support\n\n            non-misogynistic       0.67      0.71      0.69       207\nstereotype & objectification       0.65      0.66      0.66       239\n           violence or abuse       0.54      0.48      0.51       155\n\n                    accuracy                           0.63       601\n                   macro avg       0.62      0.62      0.62       601\n                weighted avg       0.63      0.63      0.63       601\n\n🧹 Cleared memory after EfficientNet\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22da3d67107e4f2cbc22a9701abf03dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bce1e700ee514d94a0bbbf05e5a084d1"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee622fb4eaab476ebcffe09de7f3cda4"
            }
          },
          "metadata": {}
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_35/2798570880.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"modify_fn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel2id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         model = AutoModelForImageClassification.from_pretrained(\n\u001b[0m\u001b[1;32m    257\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"checkpoint\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel2id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4397\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4398\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4399\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4400\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4401\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   4831\u001b[0m             \u001b[0;31m# Skip it with fsdp on ranks other than 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_local_dist_rank_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4833\u001b[0;31m                 disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m   4834\u001b[0m                     \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4835\u001b[0m                     \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0mparam_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_local_dist_rank_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m                 \u001b[0m_load_parameter_into_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_parameter_into_model\u001b[0;34m(model, param_name, tensor)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_module_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0;31m# This will check potential shape mismatch if skipped before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mparam_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Linear:\n\tsize mismatch for bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([3])."
          ],
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for Linear:\n\tsize mismatch for bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([3]).",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    random.seed(seed)\n",
        "set_seed(42)\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/kaggle/input/less-data/changed_ds/new_ds_small.csv\")\n",
        "assert not df.empty, \"Dataset is empty\"\n",
        "assert len(df) == 4001, f\"Expected 4001 samples, got {len(df)}\"\n",
        "assert df['image'].notnull().all(), \"Missing values in 'image' column\"\n",
        "assert df['label'].notnull().all(), \"Missing values in 'label' column\"\n",
        "print(\"✅ Dataset loaded. Sample:\")\n",
        "print(df.head())\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['label_id'] = label_encoder.fit_transform(df['label'])\n",
        "label2id = {label: idx for idx, label in enumerate(label_encoder.classes_)}\n",
        "id2label = {idx: label for label, idx in label2id.items()}\n",
        "print(f\"✅ Unique labels: {label2id}\")\n",
        "print(f\"Label distribution: {df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "# Verify number of classes\n",
        "num_classes = len(label2id)\n",
        "assert num_classes == 3, f\"Expected 3 classes, got {num_classes}\"\n",
        "\n",
        "# Split data into train (70%), validation (15%), test (15%)\n",
        "train_df, temp_df = train_test_split(\n",
        "    df, test_size=0.3, random_state=42, stratify=df['label_id']\n",
        ")\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.5, random_state=42, stratify=temp_df['label_id']\n",
        ")\n",
        "print(f\"✅ Dataset split: Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
        "\n",
        "# Image dataset class\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, processor=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.processor = processor\n",
        "        self.image_paths = df['image'].values\n",
        "        self.labels = df['label_id'].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.image_paths[idx])\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            image = Image.new('RGB', (224, 224))  # Fallback: blank image\n",
        "        if self.processor:\n",
        "            image = self.processor(image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return {'pixel_values': image, 'labels': label}\n",
        "\n",
        "# Model configurations\n",
        "model_configs = {\n",
        "    \"ViT\": {\n",
        "        \"checkpoint\": \"google/vit-base-patch16-224\",\n",
        "        \"use_processor\": True\n",
        "    },\n",
        "    \"SwinTransformer\": {\n",
        "        \"checkpoint\": \"microsoft/swin-base-patch4-window7-224\",\n",
        "        \"use_processor\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "# Training and evaluation function\n",
        "def train_and_evaluate(model_name, model, train_loader, val_loader, test_loader, num_classes, device):\n",
        "    print(f\"\\n🔄 Training {model_name}...\")\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 3\n",
        "    min_delta = 0.01\n",
        "    best_f1 = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    num_epochs = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        # Progress bar for training\n",
        "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\")\n",
        "        for batch in train_bar:\n",
        "            images = batch['pixel_values'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            logits = outputs.logits\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            train_bar.set_postfix({\"loss\": running_loss / len(train_bar)})\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_preds, val_labels = [], []\n",
        "        val_loss = 0.0\n",
        "        # Progress bar for validation\n",
        "        val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\")\n",
        "        with torch.no_grad():\n",
        "            for batch in val_bar:\n",
        "                images = batch['pixel_values'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "                outputs = model(images)\n",
        "                logits = outputs.logits\n",
        "                loss = criterion(logits, labels)\n",
        "                val_loss += loss.item()\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                val_preds.extend(preds.cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "                val_bar.set_postfix({\"val_loss\": val_loss / len(val_bar)})\n",
        "\n",
        "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
        "        print(f\"Epoch {epoch+1}: Val F1 = {val_f1:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_f1 > best_f1 + min_delta:\n",
        "            best_f1 = val_f1\n",
        "            patience_counter = 0\n",
        "            os.makedirs(f\"./models/{model_name}\", exist_ok=True)\n",
        "            torch.save(model.state_dict(), f\"./models/{model_name}/best_model.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load(f\"./models/{model_name}/best_model.pt\"))\n",
        "\n",
        "    # Validation evaluation\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    # Progress bar for validation evaluation\n",
        "    val_bar = tqdm(val_loader, desc=\"Validation Evaluation\")\n",
        "    with torch.no_grad():\n",
        "        for batch in val_bar:\n",
        "            images = batch['pixel_values'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(images)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(f\"\\n📊 Validation Classification Report for {model_name}\")\n",
        "    val_report = classification_report(val_labels, val_preds, target_names=label_encoder.classes_)\n",
        "    print(val_report)\n",
        "\n",
        "    # Test evaluation\n",
        "    test_preds, test_labels = [], []\n",
        "    # Progress bar for test evaluation\n",
        "    test_bar = tqdm(test_loader, desc=\"Test Evaluation\")\n",
        "    with torch.no_grad():\n",
        "        for batch in test_bar:\n",
        "            images = batch['pixel_values'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(images)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            test_preds.extend(preds.cpu().numpy())\n",
        "            test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(f\"\\n📊 Test Classification Report for {model_name}\")\n",
        "    test_report = classification_report(test_labels, test_preds, target_names=label_encoder.classes_)\n",
        "    print(test_report)\n",
        "\n",
        "    # Save results\n",
        "    results[model_name] = {\n",
        "        \"val_report\": classification_report(val_labels, val_preds, target_names=label_encoder.classes_, output_dict=True),\n",
        "        \"test_report\": classification_report(test_labels, test_preds, target_names=label_encoder.classes_, output_dict=True)\n",
        "    }\n",
        "\n",
        "    # Clear memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"🧹 Cleared memory after {model_name}\")\n",
        "\n",
        "# Main execution\n",
        "results = {}\n",
        "model_names = [\"ViT\", \"SwinTransformer\"]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create datasets and loaders\n",
        "img_dir = \"/kaggle/input/less-data/changed_ds/img\"\n",
        "for model_name in model_names:\n",
        "    config = model_configs[model_name]\n",
        "    processor = AutoImageProcessor.from_pretrained(config[\"checkpoint\"])\n",
        "    train_dataset = ImageDataset(train_df, img_dir, processor=processor)\n",
        "    val_dataset = ImageDataset(val_df, img_dir, processor=processor)\n",
        "    test_dataset = ImageDataset(test_df, img_dir, processor=processor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    # Initialize model with ignore_mismatched_sizes=True\n",
        "    model = AutoModelForImageClassification.from_pretrained(\n",
        "        config[\"checkpoint\"],\n",
        "        num_labels=num_classes,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id,\n",
        "        ignore_mismatched_sizes=True  # Fix for size mismatch\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        train_and_evaluate(model_name, model, train_loader, val_loader, test_loader, num_classes, device)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Skipping {model_name} due to error: {e}\")\n",
        "\n",
        "# Compare results\n",
        "print(\"\\n📊 Model Comparison (Test Metrics)\")\n",
        "for model_name, result in results.items():\n",
        "    test_acc = result[\"test_report\"][\"accuracy\"]\n",
        "    test_f1 = result[\"test_report\"][\"weighted avg\"][\"f1-score\"]\n",
        "    print(f\"{model_name}: Accuracy = {test_acc:.4f}, F1 = {test_f1:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-24T09:29:31.935346Z",
          "iopub.execute_input": "2025-06-24T09:29:31.935689Z",
          "iopub.status.idle": "2025-06-24T09:59:07.75801Z",
          "shell.execute_reply.started": "2025-06-24T09:29:31.935667Z",
          "shell.execute_reply": "2025-06-24T09:59:07.757193Z"
        },
        "id": "aRo363d6Y88F",
        "outputId": "88dcb691-9f82-408a-e59b-2bd589d5c425",
        "colab": {
          "referenced_widgets": [
            "9c7e619e78854b96a7030371e9813d7b",
            "9a953fcb4ed94fa9b2362bab5ae3c0f3",
            "7e6a64f0e66e4ba1b923af5448dea23d"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Dataset loaded. Sample:\n      image                                               text  \\\n0  2001.jpg  আচ্ছা ভাই।\\n মেয়েদের থেকে দূরে থাকবা মেয়ের পাল...   \n1  2002.jpg  কিসের foodpanda যেখানে আমার সাদিয়া রে অর্ডার ক...   \n2  2003.jpg  উপস্থিত sir\\n Yes sir\\n Present Teacher\\n [লাব...   \n3  2004.png  আমি হিজাব চাইনি\\n তারপর আমার স্বামী আমাকে বোঝালেন   \n4  2005.jpg  বাঙ্গু (beta, omega, theta, delta) male\\nসুন্দ...   \n\n                          label  \n0  stereotype & objectification  \n1  stereotype & objectification  \n2  stereotype & objectification  \n3             violence or abuse  \n4              non-misogynistic  \n✅ Unique labels: {'non-misogynistic': 0, 'stereotype & objectification': 1, 'violence or abuse': 2}\nLabel distribution: {'stereotype & objectification': 1591, 'non-misogynistic': 1380, 'violence or abuse': 1030}\n✅ Dataset split: Train: 2800, Val: 600, Test: 601\nUsing device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\nSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n🔄 Training ViT...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 1 Training:  75%|███████▍  | 131/175 [01:24<00:27,  1.62it/s, loss=0.727]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 1 Training: 100%|██████████| 175/175 [01:51<00:00,  1.57it/s, loss=0.951]\nEpoch 1 Validation: 100%|██████████| 38/38 [00:15<00:00,  2.50it/s, val_loss=0.921]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1: Val F1 = 0.5474\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2 Training:  73%|███████▎  | 128/175 [01:19<00:28,  1.67it/s, loss=0.418]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 2 Training: 100%|██████████| 175/175 [01:48<00:00,  1.61it/s, loss=0.557]\nEpoch 2 Validation: 100%|██████████| 38/38 [00:14<00:00,  2.64it/s, val_loss=0.897]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2: Val F1 = 0.6249\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3 Training:  90%|████████▉ | 157/175 [01:37<00:11,  1.55it/s, loss=0.166]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 3 Training: 100%|██████████| 175/175 [01:48<00:00,  1.61it/s, loss=0.191]\nEpoch 3 Validation: 100%|██████████| 38/38 [00:14<00:00,  2.59it/s, val_loss=1.04] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3: Val F1 = 0.6337\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4 Training:  31%|███▏      | 55/175 [00:34<01:11,  1.67it/s, loss=0.0244] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 4 Training: 100%|██████████| 175/175 [01:49<00:00,  1.61it/s, loss=0.0637]\nEpoch 4 Validation: 100%|██████████| 38/38 [00:14<00:00,  2.62it/s, val_loss=1.36] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4: Val F1 = 0.6026\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5 Training:  22%|██▏       | 39/175 [00:24<01:17,  1.76it/s, loss=0.00934]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 5 Training: 100%|██████████| 175/175 [01:48<00:00,  1.61it/s, loss=0.0467]\nEpoch 5 Validation: 100%|██████████| 38/38 [00:14<00:00,  2.61it/s, val_loss=1.26] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5: Val F1 = 0.6281\nEarly stopping at epoch 5\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Validation Evaluation: 100%|██████████| 38/38 [00:14<00:00,  2.59it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n📊 Validation Classification Report for ViT\n                              precision    recall  f1-score   support\n\n            non-misogynistic       0.63      0.78      0.70       207\nstereotype & objectification       0.70      0.54      0.61       239\n           violence or abuse       0.54      0.56      0.55       154\n\n                    accuracy                           0.63       600\n                   macro avg       0.62      0.63      0.62       600\n                weighted avg       0.63      0.63      0.62       600\n\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Test Evaluation: 100%|██████████| 38/38 [00:15<00:00,  2.43it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n📊 Test Classification Report for ViT\n                              precision    recall  f1-score   support\n\n            non-misogynistic       0.60      0.79      0.68       207\nstereotype & objectification       0.70      0.54      0.61       239\n           violence or abuse       0.50      0.46      0.48       155\n\n                    accuracy                           0.61       601\n                   macro avg       0.60      0.60      0.59       601\n                weighted avg       0.61      0.61      0.60       601\n\n🧹 Cleared memory after ViT\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "preprocessor_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c7e619e78854b96a7030371e9813d7b"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/71.8k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a953fcb4ed94fa9b2362bab5ae3c0f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/352M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e6a64f0e66e4ba1b923af5448dea23d"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([3, 1024]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n🔄 Training SwinTransformer...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 1 Training:  46%|████▋     | 81/175 [00:55<01:01,  1.54it/s, loss=0.452] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 1 Training: 100%|██████████| 175/175 [01:58<00:00,  1.47it/s, loss=0.955]\nEpoch 1 Validation: 100%|██████████| 38/38 [00:16<00:00,  2.30it/s, val_loss=0.881]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1: Val F1 = 0.5639\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2 Training:  28%|██▊       | 49/175 [00:33<01:25,  1.47it/s, loss=0.204] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 2 Training: 100%|██████████| 175/175 [01:58<00:00,  1.48it/s, loss=0.755]\nEpoch 2 Validation: 100%|██████████| 38/38 [00:16<00:00,  2.30it/s, val_loss=0.985]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2: Val F1 = 0.4906\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3 Training:  77%|███████▋  | 134/175 [01:30<00:27,  1.50it/s, loss=0.384]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 3 Training: 100%|██████████| 175/175 [01:58<00:00,  1.48it/s, loss=0.507]\nEpoch 3 Validation: 100%|██████████| 38/38 [00:16<00:00,  2.31it/s, val_loss=1.01] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3: Val F1 = 0.5952\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4 Training:  80%|████████  | 140/175 [01:34<00:23,  1.47it/s, loss=0.261]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 4 Training: 100%|██████████| 175/175 [01:58<00:00,  1.47it/s, loss=0.322]\nEpoch 4 Validation: 100%|██████████| 38/38 [00:16<00:00,  2.29it/s, val_loss=1.02] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 4: Val F1 = 0.6276\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5 Training:  14%|█▍        | 25/175 [00:16<01:43,  1.45it/s, loss=0.024] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 5 Training: 100%|██████████| 175/175 [01:58<00:00,  1.48it/s, loss=0.181]\nEpoch 5 Validation: 100%|██████████| 38/38 [00:16<00:00,  2.30it/s, val_loss=1.41] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 5: Val F1 = 0.6461\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6 Training:  55%|█████▌    | 97/175 [01:05<00:54,  1.44it/s, loss=0.0761] /usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 6 Training: 100%|██████████| 175/175 [01:58<00:00,  1.48it/s, loss=0.146] \nEpoch 6 Validation: 100%|██████████| 38/38 [00:16<00:00,  2.31it/s, val_loss=1.46] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 6: Val F1 = 0.6187\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7 Training:  67%|██████▋   | 118/175 [01:19<00:39,  1.43it/s, loss=0.0471]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 7 Training: 100%|██████████| 175/175 [01:58<00:00,  1.47it/s, loss=0.0915]\nEpoch 7 Validation: 100%|██████████| 38/38 [00:16<00:00,  2.29it/s, val_loss=1.3]  \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 7: Val F1 = 0.6336\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 8 Training:  81%|████████  | 142/175 [01:36<00:23,  1.38it/s, loss=0.0652]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn(\nEpoch 8 Training: 100%|██████████| 175/175 [01:58<00:00,  1.47it/s, loss=0.0891]\nEpoch 8 Validation: 100%|██████████| 38/38 [00:16<00:00,  2.25it/s, val_loss=1.44] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 8: Val F1 = 0.6180\nEarly stopping at epoch 8\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Validation Evaluation: 100%|██████████| 38/38 [00:16<00:00,  2.30it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n📊 Validation Classification Report for SwinTransformer\n                              precision    recall  f1-score   support\n\n            non-misogynistic       0.80      0.64      0.71       207\nstereotype & objectification       0.67      0.64      0.66       239\n           violence or abuse       0.48      0.64      0.55       154\n\n                    accuracy                           0.64       600\n                   macro avg       0.65      0.64      0.64       600\n                weighted avg       0.66      0.64      0.65       600\n\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Test Evaluation: 100%|██████████| 38/38 [00:17<00:00,  2.15it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n📊 Test Classification Report for SwinTransformer\n                              precision    recall  f1-score   support\n\n            non-misogynistic       0.71      0.65      0.68       207\nstereotype & objectification       0.67      0.62      0.64       239\n           violence or abuse       0.45      0.55      0.49       155\n\n                    accuracy                           0.61       601\n                   macro avg       0.61      0.61      0.61       601\n                weighted avg       0.63      0.61      0.62       601\n\n🧹 Cleared memory after SwinTransformer\n\n📊 Model Comparison (Test Metrics)\nViT: Accuracy = 0.6057, F1 = 0.6007\nSwinTransformer: Accuracy = 0.6123, F1 = 0.6172\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}